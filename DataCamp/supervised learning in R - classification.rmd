---
title: 'Supervised Learning in R: Classification'
author: "DataCamp - Brett Lantz"
date: "12/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos="https://CRAN.R-project.org")
```

## Classification with Nearest Neighbors

![](_images/1000.png)

![](_images/1001.png)

![](_images/1002.png)

![](_images/1003.png)

**Recognizing a road sign with kNN**

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

As it begins to drive away, its camera captures the following image:

![](_images/knn_stop_99.gif)

Can you apply a kNN classifier to help the car recognize this sign?

The dataset `signs` is loaded in your workspace along with the dataframe `next_sign`, which holds the observation you want to classify.

```{r}
library(tidyverse)

#prepare dataset for use (why oh why don't you do this DataCamp?)
signs <- read.csv("_data/knn_traffic_signs.csv")
next_sign <- signs %>% filter(r1 == 204 & b1 == 220) %>% select(-id, -sample, -sign_type)
signs <- signs %>% filter(sample == "train") %>% select(-id, -sample)

# Load the 'class' package
library(class)

# Create a vector of labels
sign_types <- signs$sign_type

# Classify the next sign observed
knn(train = signs[-1], test = next_sign, cl = sign_types)
```

Awesome! You've trained your first nearest neighbor classifier!

kNN isn't really learning anything; it simply looks for the most similar example.

**Exploring the traffic sign dataset**

To better understand how the `knn()` function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

![](_images/1004.png)

The result is a dataset that records the `sign_type` as well as 16 x 3 = 48 color properties of each sign.

```{r}
# Examine the structure of the signs dataset
str(signs)

# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

Great work! As you might have expected, stop signs tend to have a higher average red value. This is how kNN identifies similar signs.

**Classifying a collection of road signs**

Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

The test course includes 59 additional road signs divided into three types:

![](_images/knn_stop_28.gif) 
![](_images/knn_speed_55.gif)
![](_images/knn_peds_47.gif)

At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.

The `class` package and the dataset `signs` are already loaded in your workspace. So is the dataframe `test_signs`, which holds a set of observations you'll test your model on.

```{r}
test_signs <- read.csv("_data/knn_traffic_signs.csv")
test_signs <- test_signs %>% filter(sample == "test") %>% select(-id, -sample)

# Use kNN to identify the test road signs
sign_types <- signs$sign_type
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)

# Create a confusion matrix of the predicted versus actual values
signs_actual <- test_signs$sign_type
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

Fantastic! That self-driving car is really coming along! The confusion matrix lets you look for patterns in the classifier's errors.

## What about the 'k' in kNN?

![](_images/1005.png)

![](_images/1006.png)

With smaller neighborhoods, kNN can identify more subtle patterns in the data.

**Testing other 'k' values**

By default, the `knn()` function in the `class` package uses only the single nearest neighbor.

Setting a `k` parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare `k` values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

The `class` package is already loaded in your workspace along with the datasets `signs`, `signs_test`, and `sign_types`. The object `signs_actual` holds the true values of the signs.

```{r}
signs_test <- test_signs

# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types)
mean(signs_actual == k_1)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7)
mean(signs_actual == k_7)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 15)
mean(signs_actual == k_15)
```

You're a kNN pro! Which value of k gave the highest accuracy?

**Seeing how the neighbors voted**

When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is *any chance at all* that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the `knn()` function.

The `class` package has already been loaded in your workspace along with the datasets `signs`, `sign_types`, and `signs_test`.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7, prob = TRUE)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```

Wow! Awesome job! Now you can get an idea of how certain your kNN learner is about its classifications.

## Data preparation for kNN

![](_images/1007.png)

![](_images/1008.png)

![](_images/1009.png)

Rescaling reduces the influence of extreme values on kNN's distance function.

## Understanding Bayesian methods

![](_images/1010.png)

![](_images/1011.png)

![](_images/1012.png)

![](_images/1013.png)

**Computing probabilities**

The `where9am` data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his `location` at 9am each day as well as whether the `daytype` was a weekend or weekday.

Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.

 $P(A|B) = \frac{P(A\ and\ B)}{P(B)} $

Calculations like these are the basis of the Naive Bayes destination prediction model you'll develop in later exercises.

```{r}
locations <- read.csv("_data/locations.csv")
where9am <- locations %>% 
              filter(hour == 9) %>% 
              select(daytype, location) %>% 
              mutate(across(everything(), as.factor))

# Compute P(A) 
p_A <- nrow(subset(where9am, location == "office")) / nrow(where9am)

# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday")) / nrow(where9am)

# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday")) / nrow(where9am)

# Compute P(A | B) and print its value
p_A_given_B <- p_AB / p_B
p_A_given_B
```

Great work! In a lot of cases, calculating probabilities is as simple as counting.

**A simple Naive Bayes location model**

The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.

To see this finding in action, use the `where9am` data frame to build a Naive Bayes model on the same data.

You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?

The data frame `where9am` is available in your workspace. This dataset contains information about Brett's location at 9am on different days.

```{r}
thursday9am <- where9am[3:4,] %>% filter(daytype == "weekday") %>% select(-location)
saturday9am <- where9am[3:4,] %>% filter(daytype == "weekend") %>% select(-location)
  
# Load the naivebayes package
library(naivebayes)

# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Predict Thursday's 9am location
predict(locmodel, thursday9am)

# Predict Saturdays's 9am location
predict(locmodel, saturday9am)
```

Awesome job! Not surprisingly, Brett is most likely at the office at 9am on a Thursday, but at home at the same time on a Saturday!

**Examining "raw" probabilities**

The `naivebayes` package offers several ways to peek inside a Naive Bayes model.

Typing the name of the model object provides the `a priori` (overall) and conditional probabilities of each of the model's predictors. If one were so inclined, you might use these for calculating *posterior* (predicted) probabilities by hand.

Alternatively, R will compute the posterior probabilities for you if the `type = "prob"` parameter is supplied to the `predict()` function.

Using these methods, examine how the model's predicted 9am location probability varies from day-to-day. The model `locmodel` that you fit in the previous exercise is in your workspace.

```{r}
# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
locmodel

# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am, type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am, type = "prob")
```

Fantastic! Did you notice the predicted probability of Brett being at the office on a Saturday is zero?

One event is independent of another if knowing one doesn't give you information about how likely the other is. For example, knowing if it's raining in New York doesn't help you predict the weather in San Francisco. The weather events in the two cities are independent of each other.

## Understanding NB's "naivety"

![](_images/1014.png)

![](_images/1015.png)

The joint probability of independent events can be computed much more simply by multiplying their individual probabilities.

![](_images/1016.png)

![](_images/1017.png)

**A more sophisticated location model**

The `locations` dataset records Brett's location every hour for 13 weeks. Each hour, the tracking information includes the `daytype` (weekend or weekday) as well as the `hourtype` (morning, afternoon, evening, or night).

Using this data, build a more sophisticated model to see how Brett's predicted location not only varies by the day of week but also by the time of day. The dataset `locations` is already loaded in your workspace.

You can specify additional independent variables in your formula using the `+` sign (e.g. `y ~ x + b`).

```{r}
# The 'naivebayes' package is loaded into the workspace already
weekday_afternoon <- locations[13,]
weekday_evening <- locations[19,]

# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, data = locations)

# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)

# Predict Brett's location on a weekday evening
predict(locmodel, weekday_evening)
```

Great job! Your Naive Bayes model forecasts that Brett will be at the office on a weekday afternoon and at home in the evening.

**Preparing for unforeseen circumstances**

While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of P(office and weekend) = 0.

Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the Laplace correction will allow a small chance for these types of unforeseen circumstances.

The model `locmodel` is already in your workspace, along with the dataframe `weekend_afternoon`.

```{r}
# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built
weekend_afternoon <- locations[85,] %>% select(daytype, hourtype, location)

# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data = locations, laplace = 1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")
```

Fantastic work! Adding the Laplace correction allows for the small chance that Brett might go to the office on the weekend in the future.

**Understanding the Laplace correction**

By default, the `naive_bayes()` function in the `naivebayes` package does not use the Laplace correction. The risk of leaving this parameter unset may lead to some potential outcomes predicted to be impossible.

## Applying Naive Bayes to other problems

![](_images/1018.png)

![](_images/1019.png)

![](_images/1020.png)

## Making binary predictions with regression

![](_images/1021.png)

**Building simple logistic regression models**

The `donors` dataset contains 93,462 examples of people mailed in a fundraising solicitation for paralyzed military veterans. The `donated` column is `1` if the person made a donation in response to the mailing and `0` otherwise. This binary outcome will be the dependent variable for the logistic regression model.

The remaining columns are features of the prospective donors that may influence their donation behavior. These are the model's *independent variables*.

When building a regression model, it is often helpful to form a hypothesis about which independent variables will be predictive of the dependent variable. The `bad_address` column, which is set to `1` for an invalid mailing address and `0` otherwise, seems like it might reduce the chances of a donation. Similarly, one might suspect that religious interest (`interest_religion`) and interest in veterans affairs (`interest_veterans`) would be associated with greater charitable giving.

In this exercise, you will use these three factors to create a simple model of donation behavior. The dataset `donors` is available in your workspace.

```{r}
donors <- read.csv("_data/donors.csv", stringsAsFactors = TRUE)

# Examine the dataset to identify potential independent variables
str(donors)

# Explore the dependent variable
table(donors$donated)

# Build the donation model
donation_model <- glm(donated ~ bad_address + interest_religion + interest_veterans, 
                      data = donors, family = "binomial")

# Summarize the model results
summary(donation_model)
```

Great work! With the model built, you can now use it to make predictions!

**Making a binary prediction**

In the previous exercise, you used the `glm()` function to build a logistic regression model of donor behavior. As with many of R's machine learning methods, you can apply the `predict()` function to the model object to forecast future behavior. By default, `predict()` outputs predictions in terms of *log odds* unless `type = "response"` is specified. This converts the log odds to *probabilities*.

Because a logistic regression model estimates the *probability* of the outcome, it is up to you to determine the threshold at which the probability implies action. One must balance the extremes of being too cautious versus being too aggressive. For example, if you were to solicit only the people with a 99% or greater donation probability, you may miss out on many people with lower estimated probabilities that still choose to donate. This balance is particularly important to consider for severely imbalanced outcomes, such as in this dataset where donations are relatively rare.

The dataset `donors` and the model `donation_model` are already loaded in your workspace.

```{r}
# Estimate the donation probability
donors$donation_prob <- predict(donation_model, type = "response")

# Find the donation probability of the average prospect
mean(donors$donated)

# Predict a donation if probability of donation is greater than average
donors$donation_pred <- ifelse(donors$donation_prob > 0.0504, 1, 0)

# Calculate the model's accuracy
mean(donors$donated == donors$donation_pred)
```

Nice work! With an accuracy of nearly 80%, the model seems to be doing its job. But is it too good to be true?

With an accuracy of only 80%, the model is actually performing WORSE than if it were to predict non-donor for every record.

## Model performance tradeoffs

![](_images/1022.png)

![](_images/1023.png)

![](_images/1024.png)

When AUC values are very close, it's important to know more about how the model will be used.

**Calculating ROC Curves and AUC**

The previous exercises have demonstrated that accuracy is a very misleading measure of model performance on imbalanced datasets. Graphing the model's performance better illustrates the tradeoff between a model that is overly aggressive and one that is overly passive.

In this exercise you will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model of donations you built earlier.

The dataset `donors` with the column of predicted probabilities, `donation_prob`,is already loaded in your workspace.

```{r}
# Load the pROC package
library(pROC)

# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_prob)

# Plot the ROC curve
plot(ROC, col = "blue")

# Calculate the area under the curve (AUC)
auc(ROC)
```

Awesome job! Based on this visualization, the model isn't doing much better than baselineâ€” a model doing nothing but making predictions at random.

## Dummy variables, missing data, and interactions












